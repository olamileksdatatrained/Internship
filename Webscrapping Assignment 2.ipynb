{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5181c6ed-1ced-4f9a-aa33-0eb20fa25d0c",
   "metadata": {},
   "source": [
    "Question No 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a992ceec-63dc-4e5f-8985-f88b37e77512",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests\n",
    "!pip install beautifulsoup4\n",
    "!pip install pandas\n",
    "!pip install html5lib\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define a function to scrape job data from a given URL\n",
    "def data_job(url):\n",
    "    # Send an HTTP GET request to the URL\n",
    "    html = requests.get(url)\n",
    "  \n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(html.content, 'html5lib')\n",
    "\n",
    "    # Create an empty dictionary to store job data\n",
    "    data = {'Title': [], 'Company': [], 'Locations': [], 'Experience': []}\n",
    "\n",
    "    # Find the main div element containing job listings\n",
    "    main_div = soup.find('div', {'class': 'parentClass position-relative'})\n",
    "\n",
    "    # Loop through the job listings (limiting to 10)\n",
    "    for row in main_div:\n",
    "        # Extract job details and append to respective lists\n",
    "        data['Title'].append(row.find('h2').get_text())\n",
    "        data['Company'].append(row.find('div', {'class': 'jobCard_jobCard_cName__mYnow'}).get_text())\n",
    "        data['Locations'].append('Bangalore,' + row.find('div', {'class': 'more_info_text arrow-box down_arrow arrow-right'}).get_text())\n",
    "        data['Experience'].append(row.find('div', {'class': 'jobCard_jobCard_lists_item__YxRkV jobCard_jobIcon__3FB1t'}).get_text())\n",
    "\n",
    "    # Create a DataFrame from the collected data\n",
    "    df = pd.DataFrame(data, index=range(1, 11))\n",
    "    \n",
    "    # Print the DataFrame containing job data\n",
    "    print(df)\n",
    "\n",
    "# Entry point of the script\n",
    "if __name__ == '__main__':\n",
    "    # Define the URL of the job search page\n",
    "    url = 'https://www.shine.com/job-search/data-analyst-jobs-in-bangalore?q=Data%20Analyst&loc=Bangalore'\n",
    "    \n",
    "    # Call the data_job function with the specified URL\n",
    "    data_job(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10996e40-bfac-4374-97c0-5f38833ca5f7",
   "metadata": {},
   "source": [
    "Question No 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ccee58-a3dd-41b2-8e45-2a945d4c7a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define a function to scrape job data from a given URL\n",
    "def data_job(url):\n",
    "    # Send an HTTP GET request to the URL\n",
    "    html = requests.get(url)\n",
    "  \n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(html.content, 'html5lib')\n",
    "\n",
    "    # Create an empty dictionary to store job data\n",
    "    data = {'Title': [], 'Company': [], 'Locations': [], 'Experience': []}\n",
    "\n",
    "    # Find the main div element containing job listings\n",
    "    main_div = soup.find('div', {'class': 'parentClass position-relative'})\n",
    "\n",
    "    # Initialize a counter to limit the number of job listings\n",
    "    value_counter = 1\n",
    "  \n",
    "    # Loop through the job listings\n",
    "    for row in main_div:\n",
    "\n",
    "        # Check if the counter is less than or equal to 10\n",
    "        if value_counter <= 10:\n",
    "            # Extract job title and append to the 'Title' list\n",
    "            data['Title'].append(row.find('h2').get_text())\n",
    "            # Extract company name and append to the 'Company' list\n",
    "            data['Company'].append(row.find('div', {'class': 'jobCard_jobCard_cName__mYnow'}).get_text())\n",
    "            # Extract location and append to the 'Locations' list\n",
    "            data['Locations'].append('Bangalore,' + row.find('div', {'class': 'more_info_text arrow-box down_arrow arrow-right'}).get_text())\n",
    "            # Extract experience requirements and append to the 'Experience' list\n",
    "            data['Experience'].append(row.find('div', {'class': 'jobCard_jobCard_lists_item__YxRkV jobCard_jobIcon__3FB1t'}).get_text())\n",
    "            \n",
    "            # Increment the counter\n",
    "            value_counter += 1\n",
    "        else:\n",
    "            # Exit the loop when 10 job listings have been processed\n",
    "            break\n",
    "\n",
    "    # Create a DataFrame from the collected data\n",
    "    df = pd.DataFrame(data, index=range(1, 11))\n",
    "    \n",
    "    # Print the DataFrame containing job data\n",
    "    print(df)\n",
    "\n",
    "# Entry point of the script\n",
    "if __name__ == '__main__':\n",
    "    # Define the URL of the job search page\n",
    "    url = 'https://www.shine.com/job-search/data-scientist-jobs-in-bangalore?q=Data%20Scientist%2C%20&loc=Bangalore'\n",
    "    \n",
    "    # Call the data_job function with the specified URL\n",
    "    data_job(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feef400-fc07-4bdf-8b17-25e166ac4654",
   "metadata": {},
   "source": [
    "Question no 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4d005a-9965-464f-b4f8-7c6cacf187f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define a function to scrape job data from a given URL\n",
    "def data_job(url):\n",
    "    # Send an HTTP GET request to the URL\n",
    "    html = requests.get(url)\n",
    "  \n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(html.content, 'html5lib')\n",
    "\n",
    "    # Create an empty dictionary to store job data\n",
    "    data = {'Title': [], 'Company': [], 'Locations': [], 'Experience': []}\n",
    "\n",
    "    # Find the main div element containing job listings\n",
    "    main_div = soup.find('div', {'class': 'parentClass position-relative'})\n",
    "\n",
    "    # Initialize a counter to limit the number of job listings\n",
    "    value_counter = 1\n",
    "  \n",
    "    # Loop through the job listings\n",
    "    for row in main_div:\n",
    "\n",
    "        # Check if the counter is less than or equal to 10\n",
    "        if value_counter <= 10:\n",
    "            \n",
    "            data['Title'].append(row.find('h2').get_text())\n",
    "            \n",
    "            data['Company'].append(row.find('div', {'class': 'jobCard_jobCard_cName__mYnow'}).get_text())\n",
    "            \n",
    "            data['Locations'].append('Bangalore,' + row.find('div', {'class': 'more_info_text arrow-box down_arrow arrow-right'}).get_text())\n",
    "            \n",
    "            data['Experience'].append(row.find('div', {'class': 'jobCard_jobCard_lists_item__YxRkV jobCard_jobIcon__3FB1t'}).get_text())\n",
    "            \n",
    "            value_counter += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Create a DataFrame from the collected data\n",
    "    df = pd.DataFrame(data, index=range(1, 11))\n",
    "    \n",
    "    # Print the DataFrame containing job data\n",
    "    print(df)\n",
    "\n",
    "# Entry point of the script\n",
    "if __name__ == '__main__':\n",
    "    # Define the URL of the job search page\n",
    "    url = 'https://www.shine.com/job-search/data-scientist-jobs-in-bangalore?q=Data%20Scientist&loc=Bangalore&fsalary=1&location=406'\n",
    "    \n",
    "    # Call the data_job function with the specified URL\n",
    "    data_job(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82f06f2-667b-41be-a33f-92198ec7dfa1",
   "metadata": {},
   "source": [
    "Question no 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac8e6bf-915c-4d9d-b1ca-ba48c9dad14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define a function to scrape sunglasses data from a given URL\n",
    "def sunglasses(url, base_url):\n",
    "\n",
    "    # Create a dictionary to store sunglass data\n",
    "    data = {'Brand': [], 'Product Description': [], 'Price': []}\n",
    "\n",
    "    # Initialize a counter for the number of products scraped\n",
    "    product_count = 0\n",
    "\n",
    "    # Continue scraping until 100 products are collected\n",
    "    while product_count < 100:\n",
    "\n",
    "        # Send an HTTP GET request to the URL\n",
    "        html = requests.get(url)\n",
    "\n",
    "        # Parse the HTML content of the page using BeautifulSoup\n",
    "        soup = BeautifulSoup(html.content, 'html5lib')\n",
    "\n",
    "        # Find the main div element containing sunglass listings\n",
    "        main_div = soup.find('div', {'class': '_1YokD2 _3Mn1Gg'})\n",
    "\n",
    "        # Loop through sunglass listings\n",
    "        for row in main_div.find_all('div', {'class': '_13oc-S _1t9ceu'}):\n",
    "\n",
    "            # Check if the desired number of products is reached\n",
    "            if product_count != 100:\n",
    "                for column in row:\n",
    "                    mini_div = column.find('div', {'class': '_2B099V'})\n",
    "                    description = mini_div.find('a', {'class': 'IRpwTa'})['title']\n",
    "                    data['Product Description'].append(description)\n",
    "                    data['Brand'].append(mini_div.find('div', {'class': '_2WkVRV'}).get_text())\n",
    "                    data['Price'].append(mini_div.find('div', {'class': '_30jeq3'}).get_text())\n",
    "                    product_count += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # Update the URL to navigate to the next page of results\n",
    "        url = base_url + soup.find('a', {'class': '_1LKTO3'})['href']\n",
    "    \n",
    "    # Create a DataFrame from the collected data\n",
    "    df = pd.DataFrame(data, index=range(1, 101))\n",
    "    \n",
    "    # Print the DataFrame containing sunglass data\n",
    "    print(df)\n",
    "\n",
    "# Entry point of the script\n",
    "if __name__ == '__main__':\n",
    "    # Define the base URL for Flipkart\n",
    "    base_url = 'https://www.flipkart.com'\n",
    "\n",
    "    # Define the initial search URL for sunglasses\n",
    "    url = 'https://www.flipkart.com/search?q=sunglasse&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off'\n",
    "\n",
    "    # Call the sunglasses function with the specified URLs\n",
    "    sunglasses(url, base_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f057c4d8-c854-40c4-8a47-85ddbaafa758",
   "metadata": {},
   "source": [
    "Question No 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f235b67-44a3-4c74-8cd0-2c76d9d3239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define a function to scrape iPhone reviews from a given URL\n",
    "def iphone_reviews(base_url, url):\n",
    "\n",
    "    # Create a dictionary to store review data\n",
    "    data = {'Rating': [], 'Review Summary': [], 'Full Review': []}\n",
    "\n",
    "    # Initialize a counter for the number of reviews scraped\n",
    "    review_count = 0\n",
    "\n",
    "    # Continue scraping until 100 reviews are collected\n",
    "    while review_count < 100:\n",
    "\n",
    "        # Send an HTTP GET request to the URL\n",
    "        html = requests.get(url)\n",
    "        soup = BeautifulSoup(html.content, 'html5lib')\n",
    "\n",
    "        # Find the main div element containing review listings\n",
    "        main_div = soup.find('div', {'class': '_1YokD2 _3Mn1Gg col-9-12'})\n",
    "\n",
    "        # Loop through review listings, excluding the first and last elements\n",
    "        for row in main_div.find_all('div', {'class': '_1AtVbE col-12-12'})[2:-1]:\n",
    "            if review_count != 100:\n",
    "                # Extract rating and append to the 'Rating' list\n",
    "                data['Rating'].append(row.find('div', {'class': '_3LWZlK _1BLPMq'}).get_text())\n",
    "                # Extract review summary and append to the 'Review Summary' list\n",
    "                data['Review Summary'].append(row.find('p', {'class': '_2-N8zT'}).get_text())\n",
    "                # Extract full review and append to the 'Full Review' list\n",
    "                review = row.find('div', {'class': 't-ZTKy'}).get_text().replace('READ MORE', '')\n",
    "                data['Full Review'].append(review)\n",
    "                review_count += 1\n",
    "            else:\n",
    "                break\n",
    "        # Update the URL to navigate to the next page of reviews\n",
    "        url = base_url + soup.find('a', {'class':'_1LKTO3'})['href']\n",
    "    \n",
    "    # Create a DataFrame from the collected data\n",
    "    df = pd.DataFrame(data, index=range(1, 101))\n",
    "    \n",
    "    # Print the DataFrame containing iPhone reviews\n",
    "    print(df)\n",
    "\n",
    "# Entry point of the script\n",
    "if __name__ == '__main__':\n",
    "    # Define the base URL for Flipkart\n",
    "    base_url = 'https://www.flipkart.com'\n",
    "\n",
    "    # Define the initial URL for iPhone reviews\n",
    "    url =  'https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART'\n",
    "\n",
    "    # Call the iphone_reviews function with the specified URLs\n",
    "    iphone_reviews(base_url, url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4378993c-e6c6-4d4e-9b12-aafa760230e2",
   "metadata": {},
   "source": [
    "Question No 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9800426c-bf2b-4b0d-86b3-a2c6d1f7b04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define a function to scrape sunglasses data from a given URL\n",
    "def sneakers(url, base_url):\n",
    "\n",
    "    # Create a dictionary to store sunglass data\n",
    "    data = {'Brand': [], 'Product Description': [], 'Price': []}\n",
    "\n",
    "    # Initialize a counter for the number of products scraped\n",
    "    product_count = 0\n",
    "\n",
    "    # Continue scraping until 100 products are collected\n",
    "    while product_count < 100:\n",
    "\n",
    "        # Send an HTTP GET request to the URL\n",
    "        html = requests.get(url)\n",
    "\n",
    "        # Parse the HTML content of the page using BeautifulSoup\n",
    "        soup = BeautifulSoup(html.content, 'html5lib')\n",
    "\n",
    "        # Find the main div element containing sunglass listings\n",
    "        main_div = soup.find('div', {'class': '_1YokD2 _3Mn1Gg'})\n",
    "\n",
    "        # Loop through sunglass listings\n",
    "        for row in main_div.find_all('div', {'class': '_13oc-S'}):\n",
    "\n",
    "            # Check if the desired number of products is reached\n",
    "            if product_count != 100:\n",
    "                for column in row:\n",
    "                    mini_div = column.find('div', {'class': '_2B099V'})\n",
    "                    description = mini_div.find('a', {'class': 'IRpwTa'})['title']\n",
    "                    data['Product Description'].append(description)\n",
    "                    data['Brand'].append(mini_div.find('div', {'class': '_2WkVRV'}).get_text())\n",
    "                    data['Price'].append(mini_div.find('div', {'class': '_30jeq3'}).get_text())\n",
    "                    product_count += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # Update the URL to navigate to the next page of results\n",
    "        url = base_url + soup.find('a', {'class':'_1LKTO3'})['href']\n",
    "    \n",
    "    # Create a DataFrame from the collected data\n",
    "    df = pd.DataFrame(data, index=range(1, 101))\n",
    "    \n",
    "    # Print the DataFrame containing sunglass data\n",
    "    print(df)\n",
    "\n",
    "# Entry point of the script\n",
    "if __name__ == '__main__':\n",
    "    # Define the base URL for Flipkart\n",
    "    base_url = 'https://www.flipkart.com'\n",
    "\n",
    "    # Define the initial search URL for sunglasses\n",
    "    url = 'https://www.flipkart.com/search?q=sneakers&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off'\n",
    "\n",
    "    # Call the sunglasses function with the specified URLs\n",
    "    sneakers(url, base_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4811150-301a-4509-8fd7-31917c17c83c",
   "metadata": {},
   "source": [
    "Question No 8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72adbe2d-c611-4a3c-bdee-0dc3cf2dad8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define a function to scrape quotes data from a given URL\n",
    "def quotes(url, base_url):\n",
    "\n",
    "    # Create a dictionary to store quote data\n",
    "    data = {'Author': [], 'Quote': [], 'Type Of Quote': [] }\n",
    "\n",
    "    # Initialize a counter for the number of quotes scraped\n",
    "    quote_count = 0\n",
    "\n",
    "    # Continue scraping until 1000 quotes are collected\n",
    "    while quote_count < 1000:\n",
    "\n",
    "        # Send an HTTP GET request to the URL\n",
    "        html = requests.get(url)\n",
    "\n",
    "        # Parse the HTML content of the page using BeautifulSoup\n",
    "        soup = BeautifulSoup(html.content, 'html5lib')\n",
    "\n",
    "        # Find all div elements with the class 'wrap-block'\n",
    "        divs = soup.find_all('div', {'class': 'wrap-block'})\n",
    "\n",
    "        # Loop through quote divs\n",
    "        for div in divs:\n",
    "            # Check if the desired number of quotes is reached\n",
    "            if quote_count != 1000:\n",
    "                \n",
    "                data['Quote'].append(div.find('a', {'class': 'title'}).get_text())\n",
    "                \n",
    "                data['Author'].append(div.find('div', {'class': 'author'}).get_text())\n",
    "                \n",
    "                data['Type Of Quote'].append(div.find('div', {'class': 'tags'}).get_text())\n",
    "                \n",
    "                quote_count += 1\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        if quote_count != 1000:\n",
    "            \n",
    "            a_tag = soup.find('li', {'class': 'next'}).find('a')['href']\n",
    "            # Update the URL to navigate to the next page of quotes\n",
    "            url = base_url + a_tag\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    # Create a DataFrame from the collected data\n",
    "    df = pd.DataFrame(data, index=range(1, 1001))\n",
    "    \n",
    "    # Print the DataFrame containing quotes data\n",
    "    print(df)\n",
    "\n",
    "# Entry point of the script\n",
    "if __name__ == '__main__':\n",
    "    # Define the URL for the initial quotes page\n",
    "    url = 'https://www.azquotes.com/top_quotes.html'\n",
    "    \n",
    "    # Define the base URL for AzQuotes\n",
    "    base_url = 'https://www.azquotes.com/'\n",
    "\n",
    "    # Call the quotes function with the specified URLs\n",
    "    quotes(url, base_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f28e649-099e-44fa-b968-e84d04619d97",
   "metadata": {},
   "source": [
    "Question No 9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d14b595-6665-4756-81cc-a6fd933eefe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def prime_minister(url):\n",
    "\n",
    "    # Send an HTTP GET request to the URL\n",
    "    html = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(html.content, 'html5lib')\n",
    "\n",
    "    # Create a dictionary to store Prime Minister data\n",
    "    data = {'Name': [], 'Born-Dead': [], 'Term Of Office': [], 'Remark': []}\n",
    "\n",
    "    # Loop through table rows starting from the second row (index 1)\n",
    "    for row in soup.find('tbody').find_all('tr')[1:]:\n",
    "        \n",
    "        columns = row.find_all('td')\n",
    "        \n",
    "        data['Name'].append(columns[1].get_text())\n",
    "        \n",
    "        data['Born-Dead'].append(columns[2].get_text())\n",
    "        \n",
    "        data['Term Of Office'].append(columns[3].get_text())\n",
    "        \n",
    "        data['Remark'].append(columns[4].get_text())\n",
    "\n",
    "    # Create a DataFrame from the collected data\n",
    "    df = pd.DataFrame(data, index=range(1, 20))\n",
    "    \n",
    "    # Print the DataFrame containing Prime Minister data\n",
    "    print(df)\n",
    "\n",
    "# Entry point of the script\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    url = 'https://www.jagranjosh.com/general-knowledge/list-of-all-prime-ministers-of-india-1473165149-1'\n",
    "\n",
    "    prime_minister(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1850533-e768-4ae2-8f74-0172705de0dd",
   "metadata": {},
   "source": [
    "Question No 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7be784f-8f6a-47a2-b797-a6b362daf69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define a function to scrape data\n",
    "def expensive_cars(url):\n",
    "\n",
    "    # Send an HTTP GET request to the URL\n",
    "    html = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(html.content, 'html5lib')\n",
    "\n",
    "    # Create a dictionary to store car data\n",
    "    data = {'Car': [], 'Price': [] }\n",
    "\n",
    "    # Find the div element with class 'postBody description e-content'\n",
    "    div = soup.find('div', {'class': 'postBody description e-content'})\n",
    "\n",
    "    # Loop to get car name\n",
    "    for row in div.find_all('h3', {'class': 'subheader'})[:-1]:\n",
    "        # Append the car name to the 'Car' list\n",
    "        data['Car'].append(row.get_text())\n",
    "\n",
    "    # Loop to get car's price\n",
    "    for row in div.find_all('p', text=lambda text: text and ('Price: $' in text or '$3.0' in text)):\n",
    "        # Append the price information to the 'Price' list\n",
    "        data['Price'].append(row.get_text())\n",
    "    \n",
    "    # Create a DataFrame from the collected data\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Print the DataFrame containing car data\n",
    "    print(df)\n",
    "\n",
    "# Entry point of the script\n",
    "if __name__ == '__main__':\n",
    "    # Define the URL for the page containing the list of expensive cars\n",
    "    url = 'https://www.motor1.com/features/308149/most-expensive-new-cars-ever/'\n",
    "\n",
    "    # Call the function\n",
    "    expensive_cars(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53420cd9-0a55-4ffd-b173-9e9f5b8cc482",
   "metadata": {},
   "source": [
    "Question No 7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2ca44f-ebd7-4959-a4b0-63fc9c4ff8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "# Specify the path to the ChromeDriver executable as a string\n",
    "chromedriver_path = \"./drivers/chromedriver\"\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless=new\")\n",
    "\n",
    "service = Service(executable_path=chromedriver_path)\n",
    "\n",
    "browser = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "browser.get('https://www.alibaba.com/trade/search?fsb=y&IndexArea=product_en&keywords=laptops&knowledgeGraphId=100018292-10000728897&tab=all&')\n",
    "\n",
    "data = {'Titles': [], 'Price': [], 'Ratings': [] }\n",
    "\n",
    "div = browser.find_elements(By.CLASS_NAME, 'app-organic-search__content')\n",
    "\n",
    "divs = browser.find_elements(By.XPATH, '//div/div/div[@class=\"fy23-search-card fy23-list-card m-gallery-product-item-v2 J-search-card-wrapper\"]')\n",
    "\n",
    "for d in divs[:10]:\n",
    "    try:\n",
    "        title = d.find_element(By.XPATH, './/div/h2/a/span')\n",
    "        data['Titles'].append(title.text)\n",
    "    except NoSuchElementException:\n",
    "        continue\n",
    "    try:\n",
    "        price = d.find_element(By.XPATH, './/div/a/div[@class=\"search-card-e-price-normal margin-bottom-4\"]')\n",
    "        data['Price'].append(price.text)  # Print the text content of the price WebElement\n",
    "    except NoSuchElementException:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        ratings = d.find_element(By.XPATH, './/div/div/div/a/span[@class=\"search-card-e-review\"]')\n",
    "        data['Ratings'].append(ratings.text) \n",
    "    except NoSuchElementException:\n",
    "        continue\n",
    "    \n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "browser.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
