{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea0f7139-2242-4572-a464-b93f17c5bc00",
   "metadata": {},
   "source": [
    "1st Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2df5678e-7009-442b-bdaa-e36a64fd85fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in ./jupter-venv/lib/python3.10/site-packages (4.13.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in ./jupter-venv/lib/python3.10/site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in ./jupter-venv/lib/python3.10/site-packages (from selenium) (2.0.4)\n",
      "Requirement already satisfied: trio~=0.17 in ./jupter-venv/lib/python3.10/site-packages (from selenium) (0.22.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in ./jupter-venv/lib/python3.10/site-packages (from selenium) (2023.7.22)\n",
      "Requirement already satisfied: sniffio in ./jupter-venv/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: outcome in ./jupter-venv/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: attrs>=20.1.0 in ./jupter-venv/lib/python3.10/site-packages (from trio~=0.17->selenium) (23.1.0)\n",
      "Requirement already satisfied: idna in ./jupter-venv/lib/python3.10/site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: sortedcontainers in ./jupter-venv/lib/python3.10/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in ./jupter-venv/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.1.2)\n",
      "Requirement already satisfied: wsproto>=0.14 in ./jupter-venv/lib/python3.10/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in ./jupter-venv/lib/python3.10/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in ./jupter-venv/lib/python3.10/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Requirement already satisfied: pandas in ./jupter-venv/lib/python3.10/site-packages (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in ./jupter-venv/lib/python3.10/site-packages (from pandas) (1.25.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./jupter-venv/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./jupter-venv/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./jupter-venv/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in ./jupter-venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: requests in ./jupter-venv/lib/python3.10/site-packages (2.31.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./jupter-venv/lib/python3.10/site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./jupter-venv/lib/python3.10/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./jupter-venv/lib/python3.10/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./jupter-venv/lib/python3.10/site-packages (from requests) (3.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in ./jupter-venv/lib/python3.10/site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./jupter-venv/lib/python3.10/site-packages (from beautifulsoup4) (2.4.1)\n",
      "Requirement already satisfied: pandas in ./jupter-venv/lib/python3.10/site-packages (2.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./jupter-venv/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./jupter-venv/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in ./jupter-venv/lib/python3.10/site-packages (from pandas) (1.25.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./jupter-venv/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in ./jupter-venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: html5lib in ./jupter-venv/lib/python3.10/site-packages (1.1)\n",
      "Requirement already satisfied: six>=1.9 in ./jupter-venv/lib/python3.10/site-packages (from html5lib) (1.16.0)\n",
      "Requirement already satisfied: webencodings in ./jupter-venv/lib/python3.10/site-packages (from html5lib) (0.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n",
    "!pip install pandas\n",
    "!pip install requests\n",
    "!pip install beautifulsoup4\n",
    "!pip install pandas\n",
    "!pip install html5lib\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "chromedriver_path = \"./drivers/chromedriver\"\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless=new\")\n",
    "chrome_options.add_argument(\"--window-size=1325x744\")\n",
    "\n",
    "service = Service(executable_path=chromedriver_path)\n",
    "\n",
    "driver = webdriver.Chrome(service=service,options=chrome_options)\n",
    "\n",
    "\n",
    "driver.get('https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos')\n",
    "\n",
    "data = {'Rank': [], 'Name':[], 'Artist':[], 'Upload_Date':[], 'Views':[] }\n",
    "rows = driver.find_elements(By.TAG_NAME, 'tr')\n",
    "\n",
    "for r in rows[2:-36]:\n",
    "    columns = r.find_elements(By.TAG_NAME, 'td')\n",
    "    try:\n",
    "        columns[0].text\n",
    "    except:\n",
    "        continue\n",
    "    data['Rank'].append(columns[0].text)\n",
    "    data['Name'].append(columns[1].text)\n",
    "    data['Artist'].append(columns[2].text)\n",
    "    data['Upload_Date'].append(columns[4].text)\n",
    "    data['Views'].append(columns[3].text)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('Youtube_videos.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae395d9a-eef3-4dc4-aeec-edabd0f1b846",
   "metadata": {},
   "source": [
    "2nd Questin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53aaf1f5-f8b5-4133-aadb-91bcab68fb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "chromedriver_path = \"./drivers/chromedriver\"\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless=new\")\n",
    "chrome_options.add_argument(\"--window-size=1325x744\")\n",
    "\n",
    "service = Service(executable_path=chromedriver_path)\n",
    "\n",
    "driver = webdriver.Chrome(service=service,options=chrome_options)\n",
    "\n",
    "data = {'Series': [], 'Place': [], 'Date':[], 'Time': [] }\n",
    "\n",
    "driver.get('https://www.bcci.tv/')\n",
    "\n",
    "driver.maximize_window()\n",
    "\n",
    "close_button = driver.find_element(By.XPATH, \"//div/div/div/button[@class='close-button page-close']\")\n",
    "close_button.click()\n",
    "time.sleep(10)\n",
    "driver.find_element(By.LINK_TEXT, 'Fixtures & Results').click()\n",
    "time.sleep(20)\n",
    "main_table = driver.find_element(By.CLASS_NAME, 'tab-inner-content')\n",
    "divs = main_table.find_elements(By.XPATH, \"//div/div[@class='col-lg-12 col-md-12 col-sm-12 ng-scope']\")\n",
    "\n",
    "for div in divs:\n",
    "    try:\n",
    "        div = div.text.splitlines()\n",
    "        data['Series'].append(div[1])\n",
    "        data['Place'].append(div[4])\n",
    "        data['Date'].append(div[2])\n",
    "        data['Time'].append(div[3])\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('bcc.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5cd83c-b330-40c6-9075-d01c7feee97d",
   "metadata": {},
   "source": [
    "3rd Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aad56a62-c0f6-4252-b775-15a993d91de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "data = {'Rank' : [], 'State':[], 'GSDP (18-19)- at current prices' : [], 'GSDP(19-20)- at current prices' : [], 'Share(18-19)' : [], 'GDP($ billion)' : [] }\n",
    "\n",
    "base_url = 'https://www.statisticstimes.com/economy/'\n",
    "\n",
    "url = 'https://www.statisticstimes.com/economy/india-statistics.php/'\n",
    "\n",
    "html = requests.get(url)\n",
    "\n",
    "soap = BeautifulSoup(html.content, 'html.parser')\n",
    "\n",
    "a_tag = soap.find_all('a', {'class': 'ec'})\n",
    "\n",
    "href = a_tag[13]['href']\n",
    "\n",
    "url = base_url+href\n",
    "\n",
    "html = requests.get(url)\n",
    "\n",
    "soap = BeautifulSoup(html.content, 'html.parser')\n",
    "\n",
    "main_div = soap.find_all('div', {'class': 'fwidth'})\n",
    "\n",
    "table = main_div[2]\n",
    "\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "for row in rows[2:]:\n",
    "    columns = row.find_all('td')\n",
    "    data['Rank'].append(columns[0].text)\n",
    "    data['State'].append(columns[1].text)\n",
    "    data['GSDP(19-20)- at current prices'].append(columns[2].text)\n",
    "    data['GSDP (18-19)- at current prices'].append(columns[3].text)\n",
    "    data['Share(18-19)'].append(columns[4].text)\n",
    "    data['GDP($ billion)'].append(columns[5].text)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('IndiaGsdp.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b77e67-04db-4a66-adf6-579e7b2748d1",
   "metadata": {},
   "source": [
    "4th Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4c2f7d4-79d1-428b-8c43-d41404bc8ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "chromedriver_path = \"./drivers/chromedriver\"\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless=new\")\n",
    "chrome_options.add_argument(\"--window-size=1325x744\")\n",
    "\n",
    "service = Service(executable_path=chromedriver_path)\n",
    "\n",
    "driver = webdriver.Chrome(service=service,options=chrome_options)\n",
    "\n",
    "# Going directly to the explore page because https://github.com/ requires credentials\n",
    "\n",
    "driver.get('https://github.com/explore')\n",
    "\n",
    "data = {'Repository title' : [],'Repository description' : [], 'Contributors count': [], 'Language' : [] }\n",
    "\n",
    "driver.maximize_window()\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "driver.find_element(By.LINK_TEXT, 'Trending').click()\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "main_div = driver.find_element(By.CLASS_NAME, 'Box')\n",
    "\n",
    "divs = main_div.find_elements(By.CLASS_NAME, 'Box-row')\n",
    "\n",
    "for div in divs:\n",
    "\n",
    "    try:\n",
    "        repo = div.find_element(By.CLASS_NAME, 'Link')\n",
    "        data['Repository title'].append(repo.text)\n",
    "    except:\n",
    "        data['Repository title'].append(None)\n",
    "\n",
    "    try:\n",
    "        repo_dis = div.find_element(By.TAG_NAME, 'p')\n",
    "        data['Repository description'].append(repo_dis.text)\n",
    "    except:\n",
    "        data['Repository description'].append(None)\n",
    "    \n",
    "    try:\n",
    "        lang = div.find_elements(By.TAG_NAME, 'span')\n",
    "        data['Language'].append(lang[2].text)\n",
    "    except:\n",
    "        data['Language'].append(None)\n",
    "    try:\n",
    "        cont = div.find_elements(By.TAG_NAME, 'a')\n",
    "        data['Contributors count'].append(cont[3].text)\n",
    "    except:\n",
    "        data['Contributors count'].append(None)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('Github.csv', index=False)\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1103eabf-898f-4bb4-82eb-491397de38ac",
   "metadata": {},
   "source": [
    "6th Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43b1bbe8-0cf1-470d-8eec-f04ae5fabd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "chromedriver_path = \"./drivers/chromedriver\"\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless=new\")\n",
    "chrome_options.add_argument(\"--window-size=1325x744\")\n",
    "\n",
    "service = Service(executable_path=chromedriver_path)\n",
    "\n",
    "driver = webdriver.Chrome(service=service,options=chrome_options)\n",
    "\n",
    "data = {'Rank' : [], 'Title' : [], 'Author' : [], 'Volume Sales' : [], 'Publisher' : [], 'Genre' : [] }\n",
    "driver.get('https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare')\n",
    "\n",
    "driver.maximize_window()\n",
    "\n",
    "table = driver.find_element(By.TAG_NAME, 'tbody')\n",
    "table_rows = table.find_elements(By.TAG_NAME, 'tr')\n",
    "\n",
    "count = 0\n",
    "\n",
    "for row in table_rows:\n",
    "\n",
    "    data['Rank'].append(row.find_element(By.ID, f'table-cell-10943-{count}-0').text)\n",
    "    data['Title'].append(row.find_element(By.ID, f'table-cell-10943-{count}-1').text)\n",
    "    data['Author'].append(row.find_element(By.ID, f'table-cell-10943-{count}-2').text)\n",
    "    data['Volume Sales'].append(row.find_element(By.ID, f'table-cell-10943-{count}-3').text)\n",
    "    data['Publisher'].append(row.find_element(By.ID, f'table-cell-10943-{count}-4').text)\n",
    "    data['Genre'].append(row.find_element(By.ID, f'table-cell-10943-{count}-5').text)\n",
    "    count+=1\n",
    "    \n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('Books.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c3ad85-58e1-44c3-8aca-e2917daedcd1",
   "metadata": {},
   "source": [
    "7th Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "060cc623-7113-43b0-9438-b8fb7fcc01f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "chromedriver_path = \"./drivers/chromedriver\"\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless=new\")\n",
    "chrome_options.add_argument(\"--window-size=1325x744\")\n",
    "\n",
    "service = Service(executable_path=chromedriver_path)\n",
    "\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "data = { 'Name' : [], 'Year span' : [], 'Genre' : [], 'Run time' : [], 'Ratings' : [], 'Votes' : [] }\n",
    "\n",
    "driver.get('https://www.imdb.com/list/ls095964455/')\n",
    "\n",
    "driver.maximize_window()\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "main_div = driver.find_element(By.ID, 'main')\n",
    "\n",
    "divs = main_div.find_elements(By.XPATH, '//div/div/div/div[@class=\"lister-item mode-detail\"]')\n",
    "\n",
    "for div in divs:\n",
    "\n",
    "    Name_YearSpan = div.find_element(By.CLASS_NAME, 'lister-item-header').text.split('(')\n",
    "    Rating = div.find_element(By.CLASS_NAME, 'ipl-rating-widget').text.split()\n",
    "    Name = Name_YearSpan[0]\n",
    "    Year_Span = Name_YearSpan[1]\n",
    "    P_tags = div.find_elements(By.TAG_NAME, 'p')\n",
    "    Genre_Runtime = P_tags[0].text.split('|')\n",
    "    votes = P_tags[3].text.split(':')[1]\n",
    "    Run_Time= Genre_Runtime[1]\n",
    "    Genre = Genre_Runtime[2]\n",
    "    data['Name'].append(Name)\n",
    "    data['Year span'].append('('+Year_Span)\n",
    "    data['Ratings'].append(Rating[0])\n",
    "    data['Genre'].append(Genre)\n",
    "    data['Run time'].append(Run_Time)\n",
    "    data['Votes'].append(votes)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('Movies.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dab3f2f-f661-4ee2-90ef-81793bf6b055",
   "metadata": {},
   "source": [
    "8th Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f421d71d-21b1-4384-9cef-40fc24131e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Dataset name                  Data type  \\\n",
      "0                                  Iris                    Tabular   \n",
      "1                         Heart Disease               Multivariate   \n",
      "2                                 Adult               Multivariate   \n",
      "3                                  Wine                    Tabular   \n",
      "4  Breast Cancer Wisconsin (Diagnostic)               Multivariate   \n",
      "5                              Diabetes  Multivariate, Time-Series   \n",
      "6                      Dry Bean Dataset               Multivariate   \n",
      "7                        Car Evaluation               Multivariate   \n",
      "8            Rice (Cammeo and Osmancik)               Multivariate   \n",
      "9                          Wine Quality               Multivariate   \n",
      "\n",
      "                         Task              Attribute type No of instances  \\\n",
      "0              Classification                        Real             150   \n",
      "1              Classification  Categorical, Integer, Real             303   \n",
      "2              Classification        Categorical, Integer           48842   \n",
      "3              Classification               Integer, Real             178   \n",
      "4              Classification                        Real             569   \n",
      "5                           -        Categorical, Integer               -   \n",
      "6              Classification               Integer, Real           13611   \n",
      "7              Classification                 Categorical            1728   \n",
      "8              Classification                        Real            3810   \n",
      "9  Classification, Regression                        Real            4898   \n",
      "\n",
      "  No of attribute                   Year  \n",
      "0               4   Donated on 6/30/1988  \n",
      "1              13   Donated on 6/30/1988  \n",
      "2              14   Donated on 4/30/1996  \n",
      "3              13   Donated on 6/30/1991  \n",
      "4              30  Donated on 10/31/1995  \n",
      "5              20                   None  \n",
      "6              16   Donated on 9/13/2020  \n",
      "7               6   Donated on 5/31/1997  \n",
      "8               7   Donated on 10/5/2019  \n",
      "9              11   Donated on 10/6/2009  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "base_url = 'https://archive.ics.uci.edu'\n",
    "\n",
    "html = requests.get(base_url)\n",
    "soup = BeautifulSoup(html.content,'html.parser')\n",
    "a_tag = soup.find('a', {'class': 'btn-primary btn'})\n",
    "href = a_tag['href']\n",
    "\n",
    "url = base_url+href\n",
    "\n",
    "data = { 'Dataset name':[], 'Data type' : [], 'Task' : [], 'Attribute type' : [], 'No of instances' : [], 'No of attribute': [], 'Year' : [] }\n",
    "\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.content, 'html.parser')\n",
    "\n",
    "main_div = soup.find('div', {'class': 'flex flex-col gap-1'})\n",
    "\n",
    "divs = main_div.find_all('div', {'class': 'rounded-box bg-base-100'})\n",
    "\n",
    "for div in divs:\n",
    "    a_tag = div.find('a', {'class': 'link-hover link text-xl font-semibold'})\n",
    "    href = a_tag['href']\n",
    "    url = base_url+href\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.content, 'html.parser')\n",
    "    Name = soup.find('h1', {'class':'text-3xl font-semibold text-primary-content'}).text\n",
    "    try:\n",
    "        year=soup.find('h2', {'class': 'text-sm text-primary-content'}).text\n",
    "    except:\n",
    "        year=None\n",
    "    main_div = soup.find('div', {'class': 'grid grid-cols-8 gap-4 md:grid-cols-12'})\n",
    "    divs = main_div.find_all('div', {'class': 'col-span-4'})\n",
    "    inside_div = []\n",
    "    for div in divs:\n",
    "        content = div.find('p', {'class': 'text-md'}).text\n",
    "        inside_div.append(content)\n",
    "        \n",
    "        try:\n",
    "            data_type =inside_div[0]\n",
    "        except:\n",
    "            data_type = None\n",
    "        try:\n",
    "            task = inside_div[2]\n",
    "        except:\n",
    "            task = None\n",
    "        try:\n",
    "            instances=inside_div[4]\n",
    "        except:\n",
    "            instances=None\n",
    "        try:\n",
    "            attributes=inside_div[5]\n",
    "        except:\n",
    "            attributes=None\n",
    "        try:\n",
    "            attribute_type=inside_div[3]\n",
    "        except:\n",
    "            attribute_type=None\n",
    "        \n",
    "    data['Year'].append(year)\n",
    "    data['Dataset name'].append(Name)\n",
    "    data['Data type'].append(data_type)\n",
    "    data['No of attribute'].append(attributes)\n",
    "    data['No of instances'].append(instances)\n",
    "    data['Task'].append(task)\n",
    "    data['Attribute type'].append(attribute_type)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "df.to_csv('Dataset.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
